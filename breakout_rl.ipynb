{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f176292a",
   "metadata": {},
   "source": [
    "## Training an agent to play Breakout using Reinforcement Learning\n",
    "**Gabriel C. Ullmann, COMP 6321**\n",
    "\n",
    "In this project, I use three Reinforcement Learning algorithms (PPO, A2C, and DQN) to train an OpenAI Gym agent to play the game Breakout. Agents were trained using different combinations of algorithms, training steps, and reward functions to determine which one reaches the maximum average score and number of lives in the game. In this notebook, I will show how to create an agent, how the agent communicates with the game in order to play it and learn, and how we can understand the training process and the performance of our agent.\n",
    "\n",
    "**Run the code** cell below to import the required packages for creating an agent:\n",
    "- **OpenAI Gym**: provides us with a toolkit to build an agent.\n",
    "- **Stable Baselines3**: provides us with implementations of RL algorithms to train the agent.\n",
    "- **datetime**: for getting the current timestamp to record our agent tests.\n",
    "- **Numpy**: used only briefly for array manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266e093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594c35c",
   "metadata": {},
   "source": [
    "**Run the code** cell below to import the required packages to run the game. \n",
    "\n",
    "The implementation of the game Breakout that will be played by our agent was developed by [John Cheetham](https://github.com/johncheetham/breakout) using [pygame](https://github.com/pygame/pygame/), a popular game development library. Besides pygame, we will also need:\n",
    "- **random**: used to randomize the starting position of the ball in the game. This makes the game a bit less \"predictable\" and therefore allows us to check if our agent is learning to adapt to different situations and not just repeating the same actions.\n",
    "- **GameObjects**: a script written by me that contains classes that represent in-game objects (ball, bat, etc.), as well as utility functions and initialization parameters for the game.\n",
    "\n",
    "**P.S:** you should see pygame's version being printed to the console if it loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e60dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import GameObjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e3673",
   "metadata": {},
   "source": [
    "**1.1 Creating the game class:** I organized the entire game logic inside a class (BreakoutGame), and my first idea was simply importing this class into the notebook. However, there was an [issue](https://stackoverflow.com/questions/58687829/why-does-my-jupyter-notebook-keeps-crashing-when-rendering-text-in-pygame) with this approach: the game runs, but the execution is not terminated by calling pygame.quit(). I also tried calling sys.exit(), but then it would crash Jupyter Notebook's kernel too. Therefore, the only way I found to make this project work inside a notebook was by copying the entire class to the cell below. \n",
    "\n",
    "Since the focus of this project is Reinforcement Learning, I will not go into detail on how the game works. For brevity, I also removed code comments and documentation, but you can go directly to source to read these in more detail if you want. However, it is relevant to say that the game works in conjunction with the gym environment (BreakoutAgent) using the [Observer pattern](https://refactoring.guru/design-patterns/observer): at every step of execution, the game notifies changes to its current state (e.g position of the ball and bat, score, etc.) to the agent, which uses these observations to learn and choose its next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb8b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutGame():\n",
    "    def __init__(self):\n",
    "        self._observers = []\n",
    "\n",
    "    def attach(self, observer: GameObjects.Observer) -> None:\n",
    "        self._observers.append(observer)\n",
    "\n",
    "    def detach(self,observer: GameObjects.Observer) -> None:\n",
    "        self._observers.remove(observer)\n",
    "\n",
    "    def notify(self, event: GameObjects.Event) -> None:\n",
    "        for observer in self._observers:\n",
    "            observer.update(event)\n",
    "\n",
    "    def init_game(self):\n",
    "        self.score = 0  \n",
    "        self.wall = None\n",
    "        self.ball_xspeed = GameObjects.BALL_XSPEED\n",
    "        self.ball_yspeed = GameObjects.BALL_YSPEED\n",
    "        self.lives = GameObjects.MAX_LIVES\n",
    "        self.bat_speed = GameObjects.BAT_XSPEED\n",
    "        self.size = self.width, self.height = 640, 480\n",
    "        self.gameScreen = None\n",
    "        self.gameClock = None\n",
    "\n",
    "        self.init_graphics()\n",
    "        self.init_objects()\n",
    "        event = GameObjects.Event(self.score, self.lives, self.bat, self.ball)\n",
    "        self.notify(event)\n",
    "\n",
    "    def init_graphics(self):\n",
    "        pygame.init()  \n",
    "        self.gameScreen = pygame.display.set_mode(self.size)\n",
    "        self.gameClock = pygame.time.Clock()\n",
    "        pygame.mouse.set_visible(0) \n",
    "\n",
    "    def init_objects(self):\n",
    "        self.wall = GameObjects.Wall()\n",
    "        self.wall.build_wall(self.width)\n",
    "        self.bat = GameObjects.Bat()\n",
    "        self.ball = GameObjects.Ball()\n",
    "        self.bat.rect = self.bat.rect.move((self.width / 2) - (self.bat.rect.right / 2), self.height - 20)\n",
    "        self.ball.rect = self.ball.rect.move((self.width / 2) + random.randint(-200, 200), self.height / 2)\n",
    "\n",
    "    def run_logic(self, comm):\n",
    "        self.check_agent_inputs(comm)\n",
    "        self.check_bat_collision()  \n",
    "        self.move_ball()  \n",
    "        self.check_game_over_condition()  \n",
    "        self.check_ball_out_bounds()  \n",
    "        self.check_ball_hit_wall()    \n",
    "        self.check_quit() \n",
    "        event = GameObjects.Event(self.score, self.lives, self.bat, self.ball)\n",
    "        self.notify(event)\n",
    "\n",
    "    def check_agent_inputs(self, comm):\n",
    "        if comm == 0:                        \n",
    "            self.bat.rect = self.bat.rect.move(-self.bat_speed, 0)     \n",
    "            if (self.bat.rect.left < 0):                           \n",
    "                self.bat.rect.left = 0      \n",
    "        if comm == 1:                    \n",
    "            self.bat.rect = self.bat.rect.move(self.bat_speed, 0)\n",
    "            if (self.bat.rect.right > self.width):                            \n",
    "                self.bat.rect.right = self.width\n",
    "                      \n",
    "    def check_bat_collision(self):\n",
    "        if self.ball.isCollided(self.bat.rect):\n",
    "            self.ball_yspeed = -self.ball_yspeed                              \n",
    "            offset = self.ball.rect.center[0] - self.bat.rect.center[0]                                             \n",
    "            if offset > 0:\n",
    "                if offset > 30:  \n",
    "                    self.ball_xspeed = 7\n",
    "                elif offset > 23:                 \n",
    "                    self.ball_xspeed = 6\n",
    "                elif offset > 17:\n",
    "                    self.ball_xspeed = 5 \n",
    "            else:  \n",
    "                if offset < -30:                             \n",
    "                    self.ball_xspeed = -7\n",
    "                elif offset < -23:\n",
    "                    self.ball_xspeed = -6\n",
    "                elif self.ball_xspeed < -17:\n",
    "                    self.ball_xspeed = -5    \n",
    "\n",
    "    def move_ball(self):\n",
    "        self.ball.rect = self.ball.rect.move(self.ball_xspeed, self.ball_yspeed)\n",
    "\n",
    "        if self.ball.rect.left < 0 or self.ball.rect.right > self.width:\n",
    "            self.ball_xspeed = -self.ball_xspeed                         \n",
    "        if self.ball.rect.top < 0:\n",
    "            self.ball_yspeed = -self.ball_yspeed                      \n",
    "\n",
    "    def check_game_over_condition(self):\n",
    "        if self.ball.rect.top > self.height:\n",
    "            self.lives -= 1    \n",
    "            self.ball_xspeed = GameObjects.BALL_XSPEED\n",
    "            self.ball_yspeed = GameObjects.BALL_YSPEED            \n",
    "            self.ball.rect.center = self.width / 2 + random.randint(-200, 200), self.height / 3  \n",
    "\n",
    "        if self.lives == 0:    \n",
    "            event = GameObjects.Event(self.score, self.lives, self.bat, self.ball)\n",
    "            self.notify(event)\n",
    "\n",
    "    def check_ball_out_bounds(self):\n",
    "        if self.ball_xspeed < 0 and self.ball.rect.left < 0:\n",
    "            self.ball_xspeed = -self.ball_xspeed                                \n",
    "\n",
    "        if self.ball_xspeed > 0 and self.ball.rect.right > self.width:\n",
    "            self.ball_xspeed = -self.ball_xspeed                               \n",
    "\n",
    "    def check_ball_hit_wall(self):\n",
    "        index = self.ball.rect.collidelist(self.wall.brickrect)       \n",
    "        if index != -1: \n",
    "            if self.ball.rect.center[0] > self.wall.brickrect[index].right or \\\n",
    "                self.ball.rect.center[0] < self.wall.brickrect[index].left:\n",
    "                self.ball_xspeed = -self.ball_xspeed\n",
    "            else:\n",
    "                self.ball_yspeed = -self.ball_yspeed                          \n",
    "            self.wall.brickrect[index:index + 1] = []\n",
    "            self.score += 10\n",
    "        \n",
    "    def render(self):\n",
    "        self.gameClock.tick(60)\n",
    "\n",
    "        self.gameScreen.fill(GameObjects.BG_COLOR)\n",
    "        scoretext, scoretextrect = GameObjects.drawScore(self.score, self.width)\n",
    "        self.gameScreen.blit(scoretext, scoretextrect)\n",
    "\n",
    "        for i in range(0, len(self.wall.brickrect)):\n",
    "            self.gameScreen.blit(self.wall.brick, self.wall.brickrect[i])    \n",
    "\n",
    "        if self.wall.brickrect == []:              \n",
    "            self.wall.build_wall(self.width)                \n",
    "            self.ball_xspeed = GameObjects.BALL_XSPEED\n",
    "            self.ball_yspeed = GameObjects.BALL_YSPEED              \n",
    "            self.ball.rect.center = self.width / 2, self.height / 3\n",
    "        \n",
    "        self.gameScreen.blit(self.ball.sprite, self.ball.rect)\n",
    "        self.gameScreen.blit(self.bat.sprite, self.bat.rect)\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def check_quit(self):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "    \n",
    "    def main(self):\n",
    "        self.init_game()\n",
    "        while True:\n",
    "            self.run_logic(0)\n",
    "            self.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb8d17",
   "metadata": {},
   "source": [
    "**1.2 Creating an agent**: Gym works with the concept of environments, classes with a standardized interface inside of which you can implement your agent. An environment class is composed of 4 methods:\n",
    "- **Init**: The first method executed after class instantiation. Here we declare some attributes that will dictate the basic behavior of our agent, such as the number of actions and observations. Since this is a game of Breakout, there are only two possible actions: moving the bat to the left or to the right. We will observe 4 variables in our game: the (x,y) position of the bat, and the (x,y) position of the ball.\n",
    "- **Step**: This method must be called in every iteration of a loop when we are training or testing our agent. Inside of it, we check which agent actions will return positive/negative rewards, and also collect the observation that will be used to train the agents. Here, we give a positive reward if the agent makes the bat follow the ball, and also when it scores points. When it gets away from the ball, a negative reward is given. Otherwise, the reward equals zero, a neutral state.\n",
    "- **Reset**: Resets the game to its initial state. If we are executing our agent multiple times and eventually it reaches a \"game over\" state, we can use this method to restart the game and keep training.\n",
    "- **Render**: Like step, this method must be called in every iteration of a loop when we are training or testing our agent. Inside it, you can call your game-rendering logic (e.g: drawing things on the screen, checking for collisions, etc.). As we have already created our game object on **Init**, here we simply call self.game.render(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a065e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutAgent(gym.Env):\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BreakoutAgent, self).__init__()\n",
    "        number_of_actions = 2\n",
    "        number_of_observations = 4\n",
    "        self.action_space = spaces.Discrete(number_of_actions)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(number_of_observations,), dtype=np.float32)\n",
    "        self.game = BreakoutGame()\n",
    "        self.observer = GameObjects.Observer()\n",
    "        self.prevScore = 0\n",
    "        self.game.attach(self.observer)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.game.run_logic(action)\n",
    "\n",
    "        # default reward is zero\n",
    "        reward = 0\n",
    "        done = (self.observer.event.lives == 0) \n",
    "        info = {\"score\": self.observer.event.score, \"lives\": self.observer.event.lives}\n",
    "\n",
    "        ball = self.observer.event.ball.rect\n",
    "        bat = self.observer.event.bat.rect\n",
    "        dif_l = abs(ball.left - bat.left)\n",
    "        dif_r = abs(ball.right - bat.right)  \n",
    "        \n",
    "        # reward 1: follow the ball\n",
    "        if dif_l < 50 or dif_r < 50:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # reward 2: break blocks to increase the score\n",
    "        if self.observer.event.score - self.prevScore > 0:\n",
    "            reward = 100\n",
    "            \n",
    "        self.prevScore = self.observer.event.score\n",
    "        return np.array([ball.left, ball.right, bat.left, bat.right], dtype=np.float32), reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.init_game()\n",
    "        ball = self.observer.event.ball.rect\n",
    "        bat = self.observer.event.bat.rect\n",
    "        return np.array([ball.left, ball.right, bat.left, bat.right], dtype=np.float32)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        self.game.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e683ba3",
   "metadata": {},
   "source": [
    "**1.2 Training the agent**: \n",
    "1. Create an instance of the Gym environment class.\n",
    "1. Create an instance of the StableBaselines3' PPO algorithm, passing the environment as a parameter. Keep verbose=1 so you can observe the statistic outputted by the agent as it is trained.\n",
    "1. Call model.learn() and pass the desired number of timesteps (we will use 100K since it yields good results).  In general, the longer you train your agent, the better. Here we will use the default hyperparameters, such as learning_rate=0.003. The full list is available in the [documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html).\n",
    "1. The training may take a couple of minutes. After it is finished, the trained agent will be saved to a file so we can play it back later.\n",
    "1. We close the pygame window so it does not stay running after the agent has finished playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7d556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 932      |\n",
      "|    ep_rew_mean     | -538     |\n",
      "| time/              |          |\n",
      "|    fps             | 1531     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 932         |\n",
      "|    ep_rew_mean          | -538        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1548        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012671671 |\n",
      "|    clip_fraction        | 0.0791      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.0212     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.4        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.52e+03    |\n",
      "|    ep_rew_mean          | -231        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1560        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006422047 |\n",
      "|    clip_fraction        | 0.0273      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | -0.00316    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 387         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00461    |\n",
      "|    value_loss           | 1.04e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.52e+03     |\n",
      "|    ep_rew_mean          | -231         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1564         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033106846 |\n",
      "|    clip_fraction        | 0.00635      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.665       |\n",
      "|    explained_variance   | 0.0637       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 249          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 517          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.35e+03    |\n",
      "|    ep_rew_mean          | 654         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1567        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008487834 |\n",
      "|    clip_fraction        | 0.0381      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.0544      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 603         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    value_loss           | 1.47e+03    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "steps = 100000\n",
    "env = BreakoutAgent()\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=steps) \n",
    "model.save(\"model_test\")\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f086f3",
   "metadata": {},
   "source": [
    "**1.3 Testing the agent**: let's create a for loop and run our agent, letting it play for 2000 steps (this something is around 30s since the game runs at 60 fps).\n",
    "1. Call model.predict(), passing the observations as a parameter. As the game has just been reset, the initial observation will correspond to the initial position of the ball and bat.\n",
    "2. Call env.step() to check for rewards. The larger the reward, the more successful are the actions being taken by the agent.\n",
    "3. Call env.render() to draw on the screen and execute game logic.\n",
    "4. If the agent reaches a \"game over\" state before reaching 2000 steps, the game session will be terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31150eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "def runAgent(env, obs, model):\n",
    "    for i in range(2000):\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(\"Game over! No more lives.\")\n",
    "            break\n",
    "    return info\n",
    "            \n",
    "info = runAgent(env, obs, model)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da838ee2",
   "metadata": {},
   "source": [
    "**1.4 Analysing agent performance with tensorboard:** for PPO, A2C, DQN and other algorithms available on StableBaselines3 you can pass a folder path for the \"tensorboard_log\" parameter. When this parameter is informed, important training metrics such as policy loss and mean reward will be saved to a file that can later be read by [Tensorboard](https://www.tensorflow.org/tensorboard), a visualization tool.\n",
    "\n",
    "On the cell below, I changed the predict() function by passing the \"tensorboard_log\" parameter. Run the training again to generate the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e8d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to testing/tensorboard/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 932      |\n",
      "|    ep_rew_mean     | -678     |\n",
      "| time/              |          |\n",
      "|    fps             | 1901     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 932         |\n",
      "|    ep_rew_mean          | -678        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1491        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015061877 |\n",
      "|    clip_fraction        | 0.0783      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.0246      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98          |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00849    |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.77e+03     |\n",
      "|    ep_rew_mean          | 146          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1521         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029265007 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.676       |\n",
      "|    explained_variance   | 0.0153       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 501          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 1.32e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.77e+03    |\n",
      "|    ep_rew_mean          | 146         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1537        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012014357 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | 0.0718      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 499         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    value_loss           | 547         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.24e+03    |\n",
      "|    ep_rew_mean          | 812         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1506        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010032036 |\n",
      "|    clip_fraction        | 0.0694      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.0491      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 137         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00568    |\n",
      "|    value_loss           | 1.14e+03    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tensorboard_logs_path = 'testing/tensorboard'\n",
    "steps = 100000\n",
    "env = BreakoutAgent()\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=tensorboard_logs_path)\n",
    "model.learn(total_timesteps=steps) \n",
    "model.save(\"model_test\")\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784d3b7",
   "metadata": {},
   "source": [
    "After training, you should see inside \"testing/tensorboard\" a folder called \"PPO_1\" and a log file inside of it. If you have tensorboard installed, you can run it from inside this notebook and visualize several charts that describe the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5317711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting TensorBoard with logdir testing/tblogs/ (started 0:13:56 ago; port 6006, pid 59895).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b1972b84fe119ed3\" width=\"100%\" height=\"1000\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b1972b84fe119ed3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.display(port=6006, height=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400ce17",
   "metadata": {},
   "source": [
    "Tensorboard shows us some interesting data about the training. I will explain some of them briefly below:\n",
    "- **rollout/ep_len_mean**: Mean episode length. When this value is higher, it means our agent is playing for longer sessions, which means it is not \"dying\" in the game. If this value increases during training it is an evidence that our agent is being successful.\n",
    "- **rollout/ep_rew_mean**: Mean reward by episode. If this value increases during training it is an evidence that our agent is doing the actions we want it to do, since they return positive rewards.\n",
    "- **train/learning_rate**: Since StableBaseline's does not support adaptive learning rates, this value should stay the same throughout training.\n",
    "- **train/entropy_loss**: Entropy is way to measure \"randomness\". In the context of our agents, it indicates how random are the actions it takes. This value should decrease during training, as a sign that our agent is learning and becoming less random.\n",
    "- **train/policy_gradient_loss**: As the training progresses, this value should decrease since our agent is learning a policy that helps it maximize rewards.\n",
    "\n",
    "In sum: if the mean reward is increasing and the losses are decreasing, it is a good sign that the agent is learning how to play the game and do well on it. If it isn't, we could try with different reward approaches and hyperparameters until we found a better solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
