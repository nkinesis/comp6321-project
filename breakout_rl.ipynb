{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f176292a",
   "metadata": {},
   "source": [
    "## Training an agent to play Breakout using Reinforcement Learning\n",
    "**Gabriel C. Ullmann, COMP 6321**\n",
    "\n",
    "In this project, I use three Reinforcement Learning algorithms (PPO, A2C and DQN) to train an OpenAI Gym agent to play the game Breakout. Agents were trained using different combinations of algorithms, training steps and reward functions in order to determine which one reaches maximum average score and number of lives in the game.\n",
    "\n",
    "**Run the code** cell below to import the required packages:\n",
    "- **OpenAI Gym**: provides us with a toolkit to build and train an agent\n",
    "- **Stable Baselines3**: provides us with an implementation of a RL algorithm called PPO\n",
    "- **Datetime**: for getting the current timestamp to record our agent tests\n",
    "- **Numpy**: used only briefly for array manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266e093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594c35c",
   "metadata": {},
   "source": [
    "**Run the code** cell below to import the Breakout game that our agent will be trained to play (BreakoutGame). \n",
    "\n",
    "Additionaly, a GameObjects class must be imported, since it contains some utility methods that are used for drawing things on the screen use PyGame, controlling game variables, etc. I will not go into details about how this works since our focus here is on how the agent works, not the game. \n",
    "\n",
    "P.S: you should see pygame's version being printed to the console if this package loads correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e60dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import GameObjects\n",
    "from BreakoutGame import BreakoutGame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb8d17",
   "metadata": {},
   "source": [
    "**1.1 Creating an agent**: Gym works with the concept of environments, a class with a standardized interface inside of which you can implement your agent. An environment class is composed by 4 methods:\n",
    "- **Init**: The first method executed after class instatiation. Here we declare some attrbiutes that will dictate the basic beahvior of our agent, such as number of actions and observations. Since this is a game of Breakout, there are only two possible actions: moving the bat to the left or to the right. We will observe 4 variables in our game: the (x,y) position of the bat, and the (x,y) position of the ball.\n",
    "- **Step**: This method must be called in every iteration of a loop when we are training or testing our agent. Inside of it, we check which agent actions will return positve/negative rewards, and also collect the observation that will be used to train the agents. Here, we give o positive reward if the agent makes the bat follow the ball, and also when it scores points. When it gets away from the ball, a negative reward is given.\n",
    "- **Reset**: Resets the game to its initial state. If we are executing our agent multiple times and eventually it reaches a \"game over\" state, we can use this method to restart the game and keep training.\n",
    "- **Render**: Like step, this method must be called in every iteration of a loop when we are training or testing our agent. Inside it you can call your game-rendering logic (e.g: drawing things on the screen, checking for collisions, etc.). As we have already created our game object on **Init**, here we simply call self.game.render(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a065e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutAgent(gym.Env):\n",
    "    # Possible actions: going left or right\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BreakoutAgent, self).__init__()\n",
    "        number_of_actions = 2\n",
    "        number_of_observations = 4\n",
    "        self.action_space = spaces.Discrete(number_of_actions)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(number_of_observations,), dtype=np.float32)\n",
    "        self.game = BreakoutGame()\n",
    "        self.observer = GameObjects.Observer()\n",
    "        self.prevScore = 0\n",
    "        self.game.attach(self.observer)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.game.runLogic(action)\n",
    "\n",
    "        # by default, reward is zero; if the agent does something we want, we raise this reward to a positive value\n",
    "        reward = 0\n",
    "        \n",
    "        # indicates whether the agent reached a \"game over\" state, no more lives\n",
    "        done = (self.observer.event.lives == 0) \n",
    "        \n",
    "        # returns current score and lives, will be useful later for recording the agent's performance\n",
    "        info = {\"score\": self.observer.event.score, \"lives\": self.observer.event.lives}\n",
    "\n",
    "        ball = self.observer.event.ball.rect\n",
    "        bat = self.observer.event.bat.rect\n",
    "        dif_l = abs(ball.left - bat.left)\n",
    "        dif_r = abs(ball.right - bat.right)  \n",
    "        \n",
    "        # reward 1: follow the ball\n",
    "        if dif_l < 50 or dif_r < 50:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # reward 2: break blocks to increase the score\n",
    "        if self.observer.event.score - self.prevScore > 0:\n",
    "            reward = 100\n",
    "\n",
    "        self.prevScore = self.observer.event.score\n",
    "        \n",
    "        # on each step, return: bat position, ball position, reward, done, info\n",
    "        return np.array([ball.left, ball.right, bat.left, bat.right], dtype=np.float32), reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"reset in the end\")\n",
    "        self.game.initGame()\n",
    "        ball = self.observer.event.ball.rect\n",
    "        bat = self.observer.event.bat.rect\n",
    "        return np.array([ball.left, ball.right, bat.left, bat.right], dtype=np.float32)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        self.game.render()\n",
    "        \n",
    "    def finish(self, mode='human'):\n",
    "        print(\"called\")\n",
    "        self.game.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e683ba3",
   "metadata": {},
   "source": [
    "**1.2 Training the agent**: \n",
    "1. Create an instance of the Gym environment class\n",
    "1. Create an instance of the StableBaselines3' PPO algorithm, passing the environment as a parameter. Keep verbose=1 so you can observe the statistic outputted by the agent as it is trained\n",
    "1. Call model.learn() and pass the desired number of timesteps (we will use 100K since it yields good results). This is the number of \"frames\" of your game that will be executed. In general, the longer you train your agent, the better. However, this may vary and you will have to test multiple configurations of this hyperparameter.\n",
    "1. The script will stay in the \"learn\" line until it finishes training. After it is finished, then it will go on to reset the game state and save the trained agent to a file so we can play it back later.\n",
    "\n",
    "**P.S:** StableBaselines3 will print several times to the console during training. If this bothers you, change to verbose=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7d556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset in the end\n",
      "reset in the end\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.7e+03  |\n",
      "|    ep_rew_mean     | -532     |\n",
      "| time/              |          |\n",
      "|    fps             | 1409     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "called\n",
      "called 2\n",
      "quit()\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "steps = 100\n",
    "env = BreakoutAgent()\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=steps) \n",
    "model.save(\"model_test\")\n",
    "env.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f086f3",
   "metadata": {},
   "source": [
    "**1.3 Testing the agent**: let's create a for loop and run our agent, letting it play for 2000 steps (this something is around 30s, since the game runs at 60 fps).\n",
    "1. Call model.predict(), passing the observations as parameter. As the game has just been reset, the initial observation will correspond to the default initial position of the ball and bat.\n",
    "2. Call env.step() to check if the agent did something that will return rewards to it.\n",
    "3. Call env.render() to draw the game elements on the screen and execute game logic.\n",
    "4. If the agent reaches a \"game over\" state before reaching 2000 steps, terminate the game session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31150eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset in the end\n",
      "called\n",
      "called 2\n",
      "quit()\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "def runAgent(env, obs, model):\n",
    "    for i in range(10):\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(\"Game over! No more lives.\")\n",
    "            break\n",
    "    env.finish()\n",
    "    return info\n",
    "            \n",
    "info = runAgent(env, obs, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05ff200",
   "metadata": {},
   "source": [
    "**1.4 Recording the agent's performance**: you can execute the same function we created in 1.3, but now inside of another for loop that repeats 10 times. This way, you can run your agent through 10 game sessions of 30s, and observe how it performs. At the end of each session, you can save the obtained score and number of lives to a CSV file that can be used for further analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c4e8b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called\n",
      "called 2\n",
      "quit()\n",
      "called\n",
      "called 2\n",
      "quit()\n",
      "called\n",
      "called 2\n",
      "quit()\n",
      "called\n",
      "called 2\n",
      "quit()\n",
      "called\n",
      "called 2\n",
      "quit()\n",
      "called\n",
      "called 2\n",
      "quit()\n",
      "called\n",
      "called 2\n",
      "quit()\n",
      "called\n",
      "called 2\n",
      "quit()\n",
      "called\n",
      "called 2\n",
      "quit()\n",
      "called\n",
      "called 2\n",
      "quit()\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    info = runAgent(env, obs, model)\n",
    "\n",
    "    with open(\"scores/score_example.csv\", \"a\") as file:\n",
    "        p1 = str(i)\n",
    "        p2 = str(info[\"score\"])\n",
    "        p3 = str(info[\"lives\"])\n",
    "        p4 = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        file.write(p1 + \",\" + p2 + \",\" + p3 + \",\" + p4 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8647914",
   "metadata": {},
   "source": [
    "**1.5 Analysing the agent's performance**: read csv, calculate average score and lives for the training with PPO, 100k steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "002ab350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg score: 5.00, Avg lives: 0.00\n"
     ]
    }
   ],
   "source": [
    "ds = np.loadtxt(\"scores/score_example.csv\", delimiter=',', usecols=(1,2))\n",
    "avg_score = np.round(np.average(ds[:, 1]), 2)\n",
    "avg_lives = np.round(np.average(ds[:, 0]), 2)\n",
    "print(\"Avg score: %1.2f, Avg lives: %1.2f\" % (avg_score, avg_lives))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91270ce3",
   "metadata": {},
   "source": [
    "In the actual project, this process of training/testing was repeated for each combination of algorithm/steps/reward function. You can see more details on how this was done in practice in the file: X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ef253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
