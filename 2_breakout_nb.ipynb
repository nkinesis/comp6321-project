{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f176292a",
   "metadata": {},
   "source": [
    "## Training an agent to play Breakout using Reinforcement Learning\n",
    "**Gabriel C. Ullmann, COMP 6321**\n",
    "\n",
    "In this project, I use three Reinforcement Learning algorithms (PPO, A2C, and DQN) to train an OpenAI Gym agent to play the game Breakout. Agents were trained using different combinations of algorithms, training steps, and reward functions to determine which one reaches the maximum average score and number of lives in the game. In this notebook, I will show how to create an agent, how the agent communicates with the game in order to play it and learn, and how we can understand the training process and the performance of our agent.\n",
    "\n",
    "**Run the code** cell below to import the required packages for creating an agent:\n",
    "- **OpenAI Gym**: provides us with a toolkit to build an agent.\n",
    "- **Stable Baselines3**: provides us with implementations of RL algorithms to train the agent.\n",
    "- **datetime**: for getting the current timestamp to record our agent tests.\n",
    "- **Numpy**: used only briefly for array manipulation.\n",
    "- **Tensorboard**: used for measuring and visualizing the training process. For more information, please check section 1.4 of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "266e093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594c35c",
   "metadata": {},
   "source": [
    "**Run the code** cell below to import the required packages to run the game. \n",
    "\n",
    "The implementation of the game Breakout that will be played by our agent was developed by [John Cheetham](https://github.com/johncheetham/breakout) using [pygame](https://github.com/pygame/pygame/), a popular game development library. Besides pygame, we will also need:\n",
    "- **random**: used to randomize the starting position of the ball in the game. This makes the game a bit less \"predictable\" and therefore allows us to check if our agent is learning to adapt to different situations and not just repeating the same actions.\n",
    "- **GameObjects**: a script written by me that contains classes that represent in-game objects (ball, bat, etc.), as well as utility functions and initialization parameters for the game.\n",
    "\n",
    "**P.S:** you should see pygame's version being printed to the console if it loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e60dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import game.breakout_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e3673",
   "metadata": {},
   "source": [
    "**1.1 Creating the game class:** I organized the entire game logic inside a class (BreakoutGame), and my first idea was simply importing this class into the notebook. However, there was an [issue](https://stackoverflow.com/questions/58687829/why-does-my-jupyter-notebook-keeps-crashing-when-rendering-text-in-pygame) with this approach: the game runs, but the execution is not terminated by calling pygame.quit(). I also tried calling sys.exit(), but then it would crash Jupyter Notebook's kernel too. Therefore, the only way I found to make this project work inside a notebook was by copying the entire class to the cell below. \n",
    "\n",
    "Since the focus of this project is Reinforcement Learning, I will not go into detail on how the game works. For brevity, I also removed code comments and documentation, but you can go directly to source to read these in more detail if you want. However, it is relevant to say that the game works in conjunction with the gym environment (BreakoutAgent) using the [Observer pattern](https://refactoring.guru/design-patterns/observer): at every step of execution, the game notifies changes to its current state (e.g position of the ball and bat, score, etc.) to the agent, which uses these observations to learn and choose its next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cb8b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import pygame\n",
    "import game.breakout_objects as breakout_objects\n",
    "\n",
    "\"\"\" All functions were written by Gabriel C. Ullmann, unless otherwise noted.\n",
    "Based on code from John Cheetham (2009). Source: https://github.com/johncheetham/breakout\n",
    "\"\"\"\n",
    "class BreakoutGame():\n",
    "\n",
    "    def __init__(self):\n",
    "        self._observers = []\n",
    "\n",
    "    def attach(self, observer: breakout_objects.Observer) -> None:\n",
    "        self._observers.append(observer)\n",
    "\n",
    "    def detach(self, observer: breakout_objects.Observer) -> None:\n",
    "        self._observers.remove(observer)\n",
    "\n",
    "    def notify(self, event: breakout_objects.Event) -> None:\n",
    "        for observer in self._observers:\n",
    "            observer.update(event)\n",
    "\n",
    "    def init_game(self):\n",
    "        self.score = 0\n",
    "        self.wall = None\n",
    "        self.ball_xspeed = breakout_objects.BALL_XSPEED\n",
    "        self.ball_yspeed = breakout_objects.BALL_YSPEED\n",
    "        self.lives = breakout_objects.MAX_LIVES\n",
    "        self.bat_speed = breakout_objects.BAT_XSPEED\n",
    "        self.size = self.width, self.height = 640, 480\n",
    "        self.gameScreen = None\n",
    "        self.gameClock = None\n",
    "\n",
    "        self.init_graphics()\n",
    "        self.init_objects()\n",
    "        event = breakout_objects.Event(\n",
    "            self.score, self.lives, self.bat, self.ball)\n",
    "        self.notify(event)\n",
    "\n",
    "    def init_graphics(self):\n",
    "        pygame.init()\n",
    "        self.gameScreen = pygame.display.set_mode(self.size)\n",
    "        self.gameClock = pygame.time.Clock()\n",
    "        pygame.key.set_repeat(1, 30)\n",
    "        pygame.mouse.set_visible(0)\n",
    "\n",
    "    def init_objects(self):\n",
    "        self.wall = breakout_objects.Wall()\n",
    "        self.wall.build_wall(self.width)\n",
    "        self.bat = breakout_objects.Bat()\n",
    "        self.ball = breakout_objects.Ball()\n",
    "        self.bat.rect = self.bat.rect.move(\n",
    "            (self.width / 2) - (self.bat.rect.right / 2), self.height - 20)\n",
    "        self.ball.rect = self.ball.rect.move(\n",
    "            (self.width / 2) + random.randint(-200, 200), self.height / 2)\n",
    "\n",
    "    def run_logic(self, comm):\n",
    "        self.check_agent_commands(comm)\n",
    "        self.check_human_commands(comm)\n",
    "        self.check_collision()\n",
    "        self.check_game_over_condition(comm)\n",
    "        self.update_ball_position()\n",
    "        self.check_ball_hit_wall()\n",
    "        event = breakout_objects.Event(\n",
    "            self.score, self.lives, self.bat, self.ball)\n",
    "        self.notify(event)\n",
    "\n",
    "    def check_agent_commands(self, comm):\n",
    "        if comm == 0:\n",
    "            self.bat.rect = self.bat.rect.move(-self.bat_speed, 0)\n",
    "            if (self.bat.rect.left < 0):\n",
    "                self.bat.rect.left = 0\n",
    "        if comm == 1:\n",
    "            self.bat.rect = self.bat.rect.move(self.bat_speed, 0)\n",
    "            if (self.bat.rect.right > self.width):\n",
    "                self.bat.rect.right = self.width\n",
    "\n",
    "    def check_human_commands(self, comm):\n",
    "        if comm == -1:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    sys.exit()\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_LEFT:\n",
    "                        self.bat.rect = self.bat.rect.move(-self.bat_speed, 0)\n",
    "                        if (self.bat.rect.left < 0):\n",
    "                            self.bat.rect.left = 0\n",
    "                    if event.key == pygame.K_RIGHT:\n",
    "                        self.bat.rect = self.bat.rect.move(self.bat_speed, 0)\n",
    "                        if (self.bat.rect.right > self.width):\n",
    "                            self.bat.rect.right = self.width\n",
    "                    if event.key == pygame.K_ESCAPE:\n",
    "                        sys.exit()\n",
    "\n",
    "    def check_collision(self):\n",
    "        if self.ball.is_collided(self.bat.rect):\n",
    "            self.ball_yspeed = -self.ball_yspeed\n",
    "            offset = self.ball.rect.center[0] - self.bat.rect.center[0]\n",
    "            if offset > 0:\n",
    "                if offset > 30:\n",
    "                    self.ball_xspeed = 7\n",
    "                elif offset > 23:\n",
    "                    self.ball_xspeed = 6\n",
    "                elif offset > 17:\n",
    "                    self.ball_xspeed = 5\n",
    "            else:\n",
    "                if offset < -30:\n",
    "                    self.ball_xspeed = -7\n",
    "                elif offset < -23:\n",
    "                    self.ball_xspeed = -6\n",
    "                elif self.ball_xspeed < -17:\n",
    "                    self.ball_xspeed = -5\n",
    "\n",
    "    def check_game_over_condition(self, comm):\n",
    "        if self.ball.rect.top > self.height:\n",
    "            self.lives -= 1\n",
    "            self.ball_xspeed = breakout_objects.BALL_XSPEED\n",
    "            self.ball_yspeed = breakout_objects.BALL_YSPEED\n",
    "            self.ball.rect.center = self.width / 2 + \\\n",
    "                random.randint(-200, 200), self.height / 3\n",
    "\n",
    "        if self.lives == 0:\n",
    "            event = breakout_objects.Event(\n",
    "                self.score, self.lives, self.bat, self.ball)\n",
    "            self.notify(event)\n",
    "\n",
    "            if comm == -1:\n",
    "                self.gameScreen.fill(breakout_objects.BG_COLOR)\n",
    "                self.wall.build_wall(self.width)\n",
    "                self.lives = breakout_objects.MAX_LIVES\n",
    "                self.score = 0\n",
    "\n",
    "    def update_ball_position(self):\n",
    "        if self.ball.rect.left < 0 or self.ball.rect.right > self.width:\n",
    "            self.ball_xspeed = -self.ball_xspeed\n",
    "        if self.ball.rect.top < 0:\n",
    "            self.ball_yspeed = -self.ball_yspeed\n",
    "\n",
    "        if self.ball_xspeed < 0 and self.ball.rect.left < 0:\n",
    "            self.ball_xspeed = -self.ball_xspeed\n",
    "        if self.ball_xspeed > 0 and self.ball.rect.right > self.width:\n",
    "            self.ball_xspeed = -self.ball_xspeed\n",
    "\n",
    "        self.ball.rect = self.ball.rect.move(\n",
    "            self.ball_xspeed, self.ball_yspeed)\n",
    "\n",
    "    def check_ball_hit_wall(self):\n",
    "        index = self.ball.rect.collidelist(self.wall.brickrect)\n",
    "        if index != -1:\n",
    "            if self.ball.rect.center[0] > self.wall.brickrect[index].right or \\\n",
    "                    self.ball.rect.center[0] < self.wall.brickrect[index].left:\n",
    "                self.ball_xspeed = -self.ball_xspeed\n",
    "            else:\n",
    "                self.ball_yspeed = -self.ball_yspeed\n",
    "            self.wall.brickrect[index:index + 1] = []\n",
    "            self.score += 10\n",
    "\n",
    "    def render(self):\n",
    "        self.gameClock.tick(60)\n",
    "\n",
    "        self.gameScreen.fill(breakout_objects.BG_COLOR)\n",
    "        scoretext, scoretextrect = breakout_objects.draw_text(\n",
    "            self.score, self.width)\n",
    "        self.gameScreen.blit(scoretext, scoretextrect)\n",
    "\n",
    "        for i in range(0, len(self.wall.brickrect)):\n",
    "            self.gameScreen.blit(self.wall.brick, self.wall.brickrect[i])\n",
    "\n",
    "        # if wall completely gone, rebuild it\n",
    "        if self.wall.brickrect == []:\n",
    "            self.wall.build_wall(self.width)\n",
    "            self.ball_xspeed = breakout_objects.BALL_XSPEED\n",
    "            self.ball_yspeed = breakout_objects.BALL_YSPEED\n",
    "            self.ball.rect.center = self.width / 2, self.height / 3\n",
    "\n",
    "        self.gameScreen.blit(self.ball.sprite, self.ball.rect)\n",
    "        self.gameScreen.blit(self.bat.sprite, self.bat.rect)\n",
    "        pygame.display.flip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb8d17",
   "metadata": {},
   "source": [
    "**1.2 Creating an agent**: Gym works with the concept of environments, classes with a standardized interface inside of which you can implement your agent. An environment class is composed of 4 methods:\n",
    "- **Init**: The first method executed after class instantiation. Here we declare some attributes that will dictate the basic behavior of our agent, such as the number of actions and observations. Since this is a game of Breakout, there are only two possible actions: moving the bat to the left or to the right. We will observe 4 variables in our game: the (x,y) position of the bat, and the (x,y) position of the ball.\n",
    "- **Step**: This method must be called in every iteration of a loop when we are training or testing our agent. Inside of it, we check which agent actions will return positive/negative rewards, and also collect the observation that will be used to train the agents. Here, we give a positive reward if the agent makes the bat follow the ball, and also when it scores points. When it gets away from the ball, a negative reward is given. Otherwise, the reward equals zero, a neutral state.\n",
    "- **Reset**: Resets the game to its initial state. If we are executing our agent multiple times and eventually it reaches a \"game over\" state, we can use this method to restart the game and keep training.\n",
    "- **Render**: Like step, this method must be called in every iteration of a loop when we are training or testing our agent. Inside it, you can call your game-rendering logic (e.g: drawing things on the screen, checking for collisions, etc.). As we have already created our game object on **Init**, here we simply call self.game.render(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a065e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutAgent(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BreakoutAgent, self).__init__()\n",
    "        number_of_actions = 2\n",
    "        number_of_observations = 4\n",
    "        self.action_space = spaces.Discrete(number_of_actions)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(number_of_observations,), dtype=np.float32)\n",
    "        self.game = BreakoutGame()\n",
    "        self.observer = breakout_objects.Observer()\n",
    "        self.prevScore = 0\n",
    "        self.game.attach(self.observer)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.game.run_logic(action)\n",
    "\n",
    "        # default reward is zero\n",
    "        reward = 0\n",
    "        done = (self.observer.event.lives == 0) \n",
    "        info = {\"score\": self.observer.event.score, \"lives\": self.observer.event.lives}\n",
    "\n",
    "        ball = self.observer.event.ball.rect\n",
    "        bat = self.observer.event.bat.rect\n",
    "        dif_l = abs(ball.left - bat.left)\n",
    "        dif_r = abs(ball.right - bat.right)  \n",
    "        \n",
    "        # reward 1: follow the ball\n",
    "        if dif_l < 50 or dif_r < 50:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # reward 2: break blocks to increase the score\n",
    "        if self.observer.event.score - self.prevScore > 0:\n",
    "            reward = 100\n",
    "            \n",
    "        self.prevScore = self.observer.event.score\n",
    "        return np.array([ball.left, ball.right, bat.left, bat.right], dtype=np.float32), reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.init_game()\n",
    "        ball = self.observer.event.ball.rect\n",
    "        bat = self.observer.event.bat.rect\n",
    "        return np.array([ball.left, ball.right, bat.left, bat.right], dtype=np.float32)\n",
    "\n",
    "    def render(self):\n",
    "        self.game.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e683ba3",
   "metadata": {},
   "source": [
    "**1.2 Training the agent**: \n",
    "1. Create an instance of the Gym environment class.\n",
    "1. Create an instance of the StableBaselines3' PPO algorithm, passing the environment as a parameter. Keep verbose=1 so you can observe the statistic outputted by the agent as it is trained.\n",
    "1. Call model.learn() and pass the desired number of timesteps (we will use 100K since it yields good results).  In general, the longer you train your agent, the better. Here we will use the default hyperparameters, such as learning_rate=0.003. The full list is available in the [documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html).\n",
    "1. The training may take a couple of minutes. After it is finished, the trained agent will be saved to a file so we can play it back later.\n",
    "1. We close the pygame window so it does not stay running after the agent has finished playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.08e+03 |\n",
      "|    ep_rew_mean     | -709     |\n",
      "| time/              |          |\n",
      "|    fps             | 2259     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.08e+03   |\n",
      "|    ep_rew_mean          | -735       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1697       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01619495 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.681     |\n",
      "|    explained_variance   | 0.00751    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 102        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    value_loss           | 157        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.84e+03    |\n",
      "|    ep_rew_mean          | 136         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1574        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009711066 |\n",
      "|    clip_fraction        | 0.0202      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.66       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 89.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00519    |\n",
      "|    value_loss           | 266         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.84e+03    |\n",
      "|    ep_rew_mean          | 136         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1570        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009557122 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.0624      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 479         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    value_loss           | 1.64e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.84e+03    |\n",
      "|    ep_rew_mean          | 136         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1553        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010663893 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.0359      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00491    |\n",
      "|    value_loss           | 885         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.65e+03     |\n",
      "|    ep_rew_mean          | 1.53e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1515         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053491956 |\n",
      "|    clip_fraction        | 0.0848       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.602       |\n",
      "|    explained_variance   | -0.00662     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.17e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | 0.000819     |\n",
      "|    value_loss           | 1.99e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.65e+03     |\n",
      "|    ep_rew_mean          | 1.53e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1507         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064503737 |\n",
      "|    clip_fraction        | 0.0973       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.589       |\n",
      "|    explained_variance   | 0.00475      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.71e+03     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    value_loss           | 1.83e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.65e+03    |\n",
      "|    ep_rew_mean          | 1.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1495        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011945994 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.00062     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 298         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    value_loss           | 854         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.65e+03    |\n",
      "|    ep_rew_mean          | 1.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1498        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007096061 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | -0.000455   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 497         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 779         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.65e+03    |\n",
      "|    ep_rew_mean          | 1.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1501        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016131263 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 3.42e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 400         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.000577    |\n",
      "|    value_loss           | 1.32e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.65e+03    |\n",
      "|    ep_rew_mean          | 1.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1505        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011184307 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 7.55e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00294     |\n",
      "|    value_loss           | 289         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.65e+03    |\n",
      "|    ep_rew_mean          | 1.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1512        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025467103 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | -7.7e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.984       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.0018      |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.65e+03   |\n",
      "|    ep_rew_mean          | 1.53e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1517       |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02042584 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.487     |\n",
      "|    explained_variance   | -0.00123   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.21       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00357   |\n",
      "|    value_loss           | 6.36       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.65e+03    |\n",
      "|    ep_rew_mean          | 1.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1516        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016753044 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | -0.139      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.499       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00289    |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.65e+03    |\n",
      "|    ep_rew_mean          | 1.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1511        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017242784 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | -0.0599     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0878      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 0.00621     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.65e+03   |\n",
      "|    ep_rew_mean          | 1.53e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1514       |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01425484 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.48      |\n",
      "|    explained_variance   | 0.00161    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.497      |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | 0.00277    |\n",
      "|    value_loss           | 1.41       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.75e+03    |\n",
      "|    ep_rew_mean          | 6.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1517        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006936592 |\n",
      "|    clip_fraction        | 0.0971      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.0659      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.35        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00394    |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.75e+03    |\n",
      "|    ep_rew_mean          | 6.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1516        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007057243 |\n",
      "|    clip_fraction        | 0.0881      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | 0.00246     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 78          |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00224    |\n",
      "|    value_loss           | 160         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "steps = 100000\n",
    "env = BreakoutAgent()\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=steps) \n",
    "model.save(\"training/model_test\")\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f086f3",
   "metadata": {},
   "source": [
    "**1.3 Testing the agent**: let's create a for loop and run our agent, letting it play for 2000 steps (this something is around 30s since the game runs at 60 fps).\n",
    "1. Call model.predict(), passing the observations as a parameter. As the game has just been reset, the initial observation will correspond to the initial position of the ball and bat.\n",
    "2. Call env.step() to check for rewards. The larger the reward, the more successful are the actions being taken by the agent.\n",
    "3. Call env.render() to draw on the screen and execute game logic.\n",
    "4. If the agent reaches a \"game over\" state before reaching 2000 steps, the game session will be terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31150eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "def runAgent(env, obs, model):\n",
    "    for i in range(2000):\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(\"Game over! No more lives.\")\n",
    "            break\n",
    "    return info\n",
    "            \n",
    "info = runAgent(env, obs, model)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da838ee2",
   "metadata": {},
   "source": [
    "**1.4 Analysing agent performance with tensorboard:** for PPO, A2C, DQN and other algorithms available on StableBaselines3 you can pass a folder path for the \"tensorboard_log\" parameter. When this parameter is informed, important training metrics such as policy loss and mean reward will be saved to a file that can later be read by [Tensorboard](https://www.tensorflow.org/tensorboard), a visualization tool.\n",
    "\n",
    "On the cell below, I changed the predict() function by passing the \"tensorboard_log\" parameter. Run the training again to generate the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e8d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to testing/tensorboard/PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.41e+03 |\n",
      "|    ep_rew_mean     | -63      |\n",
      "| time/              |          |\n",
      "|    fps             | 2179     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.4e+03     |\n",
      "|    ep_rew_mean          | -130        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1829        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013949985 |\n",
      "|    clip_fraction        | 0.0763      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | 0.000503    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 323         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0074     |\n",
      "|    value_loss           | 565         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.94e+03    |\n",
      "|    ep_rew_mean          | 235         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1737        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011015733 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.663      |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 166         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00558    |\n",
      "|    value_loss           | 226         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.94e+03     |\n",
      "|    ep_rew_mean          | 235          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1686         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068569747 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.648       |\n",
      "|    explained_variance   | 0.0789       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 919          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00476     |\n",
      "|    value_loss           | 714          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.94e+03    |\n",
      "|    ep_rew_mean          | 235         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1654        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011260366 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.594      |\n",
      "|    explained_variance   | 0.0702      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 143         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 564         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.95e+03    |\n",
      "|    ep_rew_mean          | 2.14e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1635        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005563696 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.026       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.83e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.000563    |\n",
      "|    value_loss           | 2.65e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.95e+03    |\n",
      "|    ep_rew_mean          | 2.14e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1592        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010731603 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.0291      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 241         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00874    |\n",
      "|    value_loss           | 1.18e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.95e+03    |\n",
      "|    ep_rew_mean          | 2.14e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1520        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009637548 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | -0.023      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 290         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.000195    |\n",
      "|    value_loss           | 1.44e+03    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.95e+03  |\n",
      "|    ep_rew_mean          | 2.14e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 1517      |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0069439 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.567    |\n",
      "|    explained_variance   | -0.0031   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 798       |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | -0.00341  |\n",
      "|    value_loss           | 1.31e+03  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.95e+03    |\n",
      "|    ep_rew_mean          | 2.14e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1510        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009679142 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.0109      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 178         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00321    |\n",
      "|    value_loss           | 864         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.95e+03    |\n",
      "|    ep_rew_mean          | 2.14e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1495        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016747437 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.0052      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00637     |\n",
      "|    value_loss           | 236         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.95e+03    |\n",
      "|    ep_rew_mean          | 2.14e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1485        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022084951 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.0139      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.95e+03    |\n",
      "|    ep_rew_mean          | 2.14e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1491        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014762957 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.00313     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 85.5        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.0013      |\n",
      "|    value_loss           | 88.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.95e+03    |\n",
      "|    ep_rew_mean          | 2.14e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1496        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017754706 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 0.0766      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.986       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00442     |\n",
      "|    value_loss           | 3.99        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.1e+03    |\n",
      "|    ep_rew_mean          | 6.12e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1501       |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02643086 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.481     |\n",
      "|    explained_variance   | 0.117      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0671     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | 0.000373   |\n",
      "|    value_loss           | 1.56       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1497        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024207052 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.455      |\n",
      "|    explained_variance   | -0.00258    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.232       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00607    |\n",
      "|    value_loss           | 32.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1497        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004258923 |\n",
      "|    clip_fraction        | 0.0632      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.000298    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 373         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.0015      |\n",
      "|    value_loss           | 1.32e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1501        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010980405 |\n",
      "|    clip_fraction        | 0.087       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | -0.000454   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 956         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.000859    |\n",
      "|    value_loss           | 1.12e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1500        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012532937 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | 2.24e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 285         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    value_loss           | 605         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1500        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013269953 |\n",
      "|    clip_fraction        | 0.0975      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.424      |\n",
      "|    explained_variance   | 2.03e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 101         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.00371     |\n",
      "|    value_loss           | 465         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.1e+03    |\n",
      "|    ep_rew_mean          | 6.12e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1495       |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02322922 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.361     |\n",
      "|    explained_variance   | -0.000101  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 37.8       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | 0.00056    |\n",
      "|    value_loss           | 80.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1495        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050970264 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.408      |\n",
      "|    explained_variance   | -0.03       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.486       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.00878     |\n",
      "|    value_loss           | 0.687       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1497        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019109186 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | -2.69       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00192     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00666    |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1492        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012983177 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | -0.00257    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.512       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.00525     |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1491        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009174332 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.457      |\n",
      "|    explained_variance   | 0.000255    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.37        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    value_loss           | 3.41        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1489        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067912534 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.352      |\n",
      "|    explained_variance   | -0.331      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | 0.00577     |\n",
      "|    value_loss           | 0.084       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1e+03     |\n",
      "|    ep_rew_mean          | 6.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1485        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036330897 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | -0.0811     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.029      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00286    |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.53e+03     |\n",
      "|    ep_rew_mean          | 1.02e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1484         |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029912628 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.461       |\n",
      "|    explained_variance   | 0.277        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | 0.00625      |\n",
      "|    value_loss           | 35.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.53e+03     |\n",
      "|    ep_rew_mean          | 1.02e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1482         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019604797 |\n",
      "|    clip_fraction        | 0.00571      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0.415        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.9         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00284     |\n",
      "|    value_loss           | 87.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.53e+03     |\n",
      "|    ep_rew_mean          | 1.02e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1473         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006889252 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.0408       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 123          |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -9.71e-05    |\n",
      "|    value_loss           | 1.25e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.53e+03   |\n",
      "|    ep_rew_mean          | 1.02e+04   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1468       |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00223516 |\n",
      "|    clip_fraction        | 0.0169     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.427     |\n",
      "|    explained_variance   | -0.0427    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 304        |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.00213   |\n",
      "|    value_loss           | 861        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.53e+03     |\n",
      "|    ep_rew_mean          | 1.02e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1472         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019489285 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | 0.00347      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 257          |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    value_loss           | 1.17e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.44e+03     |\n",
      "|    ep_rew_mean          | 1.04e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1471         |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013838342 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.438       |\n",
      "|    explained_variance   | 0.172        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 79.7         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 243          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.44e+03     |\n",
      "|    ep_rew_mean          | 1.04e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1467         |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011774895 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.408       |\n",
      "|    explained_variance   | 0.0142       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 383          |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 1.23e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1463        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011324922 |\n",
      "|    clip_fraction        | 0.0634      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | -0.0465     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 180         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 0.00635     |\n",
      "|    value_loss           | 1.12e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1464        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010204947 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.000207    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 394         |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00229    |\n",
      "|    value_loss           | 691         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1465        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010941217 |\n",
      "|    clip_fraction        | 0.0953      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.414      |\n",
      "|    explained_variance   | 0.000871    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00262    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.44e+03     |\n",
      "|    ep_rew_mean          | 1.04e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1462         |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037713416 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | 0.0368       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 228          |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00355     |\n",
      "|    value_loss           | 241          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1463        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006901945 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.0832      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 283         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0039     |\n",
      "|    value_loss           | 245         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1465        |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018157095 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.00284     |\n",
      "|    value_loss           | 5.88        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.44e+03   |\n",
      "|    ep_rew_mean          | 1.04e+04   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1465       |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 57         |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01103333 |\n",
      "|    clip_fraction        | 0.0744     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.331     |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.92       |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.000671  |\n",
      "|    value_loss           | 7.53       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1466        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006389179 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.351      |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.544       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.00361     |\n",
      "|    value_loss           | 1.24        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.44e+03     |\n",
      "|    ep_rew_mean          | 1.04e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1467         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069937226 |\n",
      "|    clip_fraction        | 0.0654       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.329       |\n",
      "|    explained_variance   | 0.337        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.19         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | 0.00126      |\n",
      "|    value_loss           | 0.571        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1469        |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009290716 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0937      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.00124     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1469        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008662063 |\n",
      "|    clip_fraction        | 0.0997      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.00314     |\n",
      "|    value_loss           | 0.247       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1470        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015354788 |\n",
      "|    clip_fraction        | 0.083       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.291       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 0.00494     |\n",
      "|    value_loss           | 0.275       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1471        |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009518221 |\n",
      "|    clip_fraction        | 0.0918      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.246      |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00475     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.44e+03    |\n",
      "|    ep_rew_mean          | 1.04e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1472        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032960165 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0212      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.00917     |\n",
      "|    value_loss           | 0.0482      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.44e+03   |\n",
      "|    ep_rew_mean          | 1.04e+04   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1474       |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 68         |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02377006 |\n",
      "|    clip_fraction        | 0.0993     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.292     |\n",
      "|    explained_variance   | 0.157      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00644    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | 0.0065     |\n",
      "|    value_loss           | 0.0198     |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tensorboard_logs_path = 'testing/tensorboard'\n",
    "steps = 100000\n",
    "env = BreakoutAgent()\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=tensorboard_logs_path)\n",
    "model.learn(total_timesteps=steps) \n",
    "model.save(\"model_test\")\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784d3b7",
   "metadata": {},
   "source": [
    "After training, you should see inside \"testing/tensorboard\" a folder called \"PPO_1\" and a log file on it. If you have tensorboard installed, you can run it in this folder by copying and pasting the command below on a terminal window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d05cb",
   "metadata": {},
   "source": [
    "```\n",
    "tensorboard --logdir testing/tensorboard\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889ebfd",
   "metadata": {},
   "source": [
    "After Tensorboard is running, you can execute the cell below to visualize several charts that describe the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5317711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting TensorBoard with logdir testing/tensorboard (started 0:10:50 ago; port 6006, pid 6874).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f8f832ebd4c61e4\" width=\"100%\" height=\"1000\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f8f832ebd4c61e4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook.display(port=6006, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400ce17",
   "metadata": {},
   "source": [
    "Tensorboard shows us some interesting data about the training. I will explain some of them briefly below:\n",
    "- **rollout/ep_len_mean**: Mean episode length. When this value is higher, it means our agent is playing for longer sessions, which means it is not \"dying\" in the game. If this value increases during training it is an evidence that our agent is being successful.\n",
    "- **rollout/ep_rew_mean**: Mean reward by episode. If this value increases during training it is an evidence that our agent is doing the actions we want it to do, since they return positive rewards.\n",
    "- **train/learning_rate**: Since StableBaseline's does not support adaptive learning rates, this value should stay the same throughout training.\n",
    "- **train/entropy_loss**: Entropy is way to measure \"randomness\". In the context of our agents, it indicates how random are the actions it takes. This value should decrease during training, as a sign that our agent is learning and becoming less random.\n",
    "- **train/loss**: the value of the total loss.\n",
    "- **train/policy_loss**: As the agent explores possibilities of action inside the game, this value should vary, since it indicates the policy is changing (valid only for on-policy algorithms).\n",
    "- **train/value_loss**: According to StableBaselines3 documentation, \"the value function loss (...) usually error between value function output and Monte-Carle (sic) estimate\". \n",
    "- **train/policy_gradient_loss**: According to StableBaselines3 documentation, \"its value does not have much meaning\" and is probably used only for debugging the algorithm itself.\n",
    "\n",
    "In sum: if the mean reward is increasing and the losses are decreasing, it is a good sign that the agent is learning how to play the game and do well on it. If it isn't, we could try with different reward approaches and hyperparameters until we found a better solution.\n",
    "\n",
    "More info on StableBaselines3 documentation: https://stable-baselines3.readthedocs.io/en/master/common/logger.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
